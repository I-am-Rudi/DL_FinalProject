{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device for all tensor calculations.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "# torch \n",
    "import torch # pytorch package, allows using GPUs\n",
    "import torch.nn.functional as F # implements forward and backward definitions of an autograd operation\n",
    "from torch import nn, optim\n",
    "from torch.nn.modules import Module\n",
    "from torchvision import datasets # load data\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "#own modules\n",
    "\n",
    "from run_training import run_analysis\n",
    "from Networks import *\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using cuda device for all tensor calculations.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using cpu device for all tensor calculations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pm, std_pm = run_analysis(plain_model, 100, 30, [64, 64, 32], device, lr=.01, BN=False, DO=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pmi, std_pmi = run_analysis(plain_model, 100, 30, [64, 64, 32], device, lr=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ws, std_ws = run_analysis(ws_model, 100, 30, [64, 64, 32], device, lr=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the following architecture:\n",
      " naive_aux_model(\n",
      "  (conv): ModuleList(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (aux): ModuleList(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4042fe816abd4517813ca253e3738b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Result] Accuracy(Training): (90.700%), Accuracy(Test): (88.800%)\n",
      "ran for a total of 30 epochs\n",
      "\n",
      " \n",
      "\n",
      "[Final Result] Accuracy(Training): (72.100%), Accuracy(Test): (72.700%)\n",
      "ran for a total of 30 epochs\n",
      "\n",
      " \n",
      "\n",
      "Epoch:  4 , Accuracy(Training): (72.800%), Accuracy(Test): (74.300%)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rudi/Documents/Uni/Courses-Exchange/DeepLearning/Final_Project/Project_1/main.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rudi/Documents/Uni/Courses-Exchange/DeepLearning/Final_Project/Project_1/main.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mean_aux, std_aux \u001b[39m=\u001b[39m run_analysis(naive_aux_model, \u001b[39m100\u001b[39;49m, \u001b[39m30\u001b[39;49m, [\u001b[39m64\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m32\u001b[39;49m], device, lr\u001b[39m=\u001b[39;49m\u001b[39m.01\u001b[39;49m, DO\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, BN\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/Uni/Courses-Exchange/DeepLearning/Final_Project/Project_1/run_training.py:103\u001b[0m, in \u001b[0;36mrun_analysis\u001b[0;34m(model, nb_trials, epochs, layers, device, batch_size, lr, loss, optimizer_name, BN, DO)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining the following architecture:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, model(layers, BN\u001b[39m=\u001b[39mBN, DO\u001b[39m=\u001b[39mDO))\n\u001b[1;32m    102\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(nb_trials)):\n\u001b[0;32m--> 103\u001b[0m     test_accuracy\u001b[39m.\u001b[39mappend(run_trial(model, epochs, layers, device, batch_size, loss, optimizer_name, lr, BN, DO))\n\u001b[1;32m    105\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mtensor(test_accuracy), \u001b[39m0\u001b[39m)\n\u001b[1;32m    106\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstd(torch\u001b[39m.\u001b[39mtensor(test_accuracy), \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Uni/Courses-Exchange/DeepLearning/Final_Project/Project_1/run_training.py:81\u001b[0m, in \u001b[0;36mrun_trial\u001b[0;34m(model, epochs, layers, device, batch_size, loss, optimizer_name, lr, BN, DO)\u001b[0m\n\u001b[1;32m     77\u001b[0m test_accuracy \u001b[39m=\u001b[39m []\n\u001b[1;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> 81\u001b[0m     train_accuracy\u001b[39m.\u001b[39mappend(Teacher\u001b[39m.\u001b[39;49mtrain(NN, batch_size))\n\u001b[1;32m     82\u001b[0m     test_l, test_acc \u001b[39m=\u001b[39m Teacher\u001b[39m.\u001b[39mtest(NN, batch_size)\n\u001b[1;32m     83\u001b[0m     test_loss\u001b[39m.\u001b[39mappend(test_l)\n",
      "File \u001b[0;32m~/Documents/Uni/Courses-Exchange/DeepLearning/Final_Project/Project_1/run_training.py:36\u001b[0m, in \u001b[0;36mteacher.train\u001b[0;34m(self, model, batch_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m inputs, targets, classes \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_data\u001b[39m.\u001b[39msplit(batch_size), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_target\u001b[39m.\u001b[39msplit(batch_size), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_classes\u001b[39m.\u001b[39msplit(batch_size)):\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39maux:\n\u001b[0;32m---> 36\u001b[0m         output, aux1, aux2 \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     37\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(output, targets) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(aux1, classes[:, \u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(aux2, classes[:, \u001b[39m1\u001b[39m])\n\u001b[1;32m     39\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Uni/Courses-Exchange/DeepLearning/Final_Project/Project_1/Networks.py:208\u001b[0m, in \u001b[0;36mnaive_aux_model.forward\u001b[0;34m(self, input_data, train)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv:\n\u001b[1;32m    207\u001b[0m     x1 \u001b[39m=\u001b[39m conv(x1)\n\u001b[0;32m--> 208\u001b[0m     x2 \u001b[39m=\u001b[39m conv(x2)\n\u001b[1;32m    211\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x1, x2), \u001b[39m1\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py:1442\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1443\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_aux, std_aux = run_analysis(naive_aux_model, 100, 30, [64, 64, 32], device, lr=.01, DO=None, BN=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9776258cb6398d27f5227b85aed097ba1d94f1dc0f3390982c9d110f6351e5c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
