{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "# torch \n",
    "import torch # pytorch package, allows using GPUs\n",
    "import torch.nn.functional as F # implements forward and backward definitions of an autograd operation\n",
    "from torch import nn, optim\n",
    "from torch.nn.modules import Module\n",
    "from torchvision import datasets # load data\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "# fix seed\n",
    "seed=12\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plain_model(nn.Module):\n",
    "    def __init__(self, lin_layers, activation = nn.ReLU(), out_activation = nn.LogSoftmax(dim=1), device=None):\n",
    "        super().__init__()\n",
    "        ########### setting up conv layers ###########\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Conv2d(1, 32, kernel_size=5))\n",
    "        self.layers.append(activation)\n",
    "        self.layers.append(nn.Conv2d(32, 64, kernel_size=5))\n",
    "        self.layers.append(activation)\n",
    "        self.layers.append(nn.MaxPool2d(2, strides = 2))\n",
    "        self.layers.append(nn.Conv2d(64, 64, kernel_size=1))\n",
    "        self.layers.append(activation)\n",
    "        self.layers.append(nn.MaxPool2d(2, strides = 2))\n",
    "        self.layers.append(nn.Flatten())\n",
    "\n",
    "        layers_full = [input_size] + lin_layers\n",
    "        \n",
    "        ################## FFN ##################  \n",
    "        # variable number of hidden layers and neurons, controlled by \n",
    "        for i, layer in enumerate(layers):\n",
    "            self.layers.append(nn.Linear(layers_full[i], layer))\n",
    "            if activation is not None:\n",
    "                assert isinstance(activation, Module), \\\n",
    "                self.layers.append(activation)\n",
    "            self.layers.append(nn.BatchNorm2d())\n",
    "\n",
    "        self.layers.append(nn.Linear(layers[-1], 2))\n",
    "        \n",
    "        # only one dropout layer after all BN layers inspired by https://arxiv.org/pdf/1801.05134.pdf (also we have a very low amount of training data)\n",
    "        self.layers.append(nn.Dropout(.25))\n",
    "\n",
    "        self.layers.append(out_activation)\n",
    "        \n",
    "        if device = None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, input_data, train=True):\n",
    "        x = test_tensor = torch.cat((input_data[:, 0, :,:], input_data[:,1, :, :]), 2)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9776258cb6398d27f5227b85aed097ba1d94f1dc0f3390982c9d110f6351e5c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
