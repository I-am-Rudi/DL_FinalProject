{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device for all tensor calculations.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "# torch \n",
    "import torch # pytorch package, allows using GPUs\n",
    "import torch.nn.functional as F # implements forward and backward definitions of an autograd operation\n",
    "from torch import nn, optim\n",
    "from torch.nn.modules import Module\n",
    "from torchvision import datasets # load data\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using cuda device for all tensor calculations.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using cpu device for all tensor calculations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plain_model(nn.Module):\n",
    "    def __init__(self, lin_layers, activation = nn.ReLU(), out_activation = nn.LogSoftmax(dim=1), device=None):\n",
    "        super().__init__()\n",
    "        ########### setting up conv layers ###########\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Conv2d(1, 32, kernel_size=5))\n",
    "        self.layers.append(activation)\n",
    "        self.layers.append(nn.Conv2d(32, 64, kernel_size=5))\n",
    "        self.layers.append(activation)\n",
    "        self.layers.append(nn.MaxPool2d(2, stride = 2))\n",
    "        self.layers.append(nn.Conv2d(64, 64, kernel_size=1))\n",
    "        self.layers.append(activation)\n",
    "        self.layers.append(nn.MaxPool2d(2, stride = 2))\n",
    "        self.layers.append(nn.Flatten())\n",
    "\n",
    "        layers_full = [320] + lin_layers\n",
    "        \n",
    "        ################## FFN ##################  \n",
    "        # variable number of hidden layers and neurons, controlled by \n",
    "        for i, layer in enumerate(lin_layers):\n",
    "            self.layers.append(nn.Linear(layers_full[i], layer))\n",
    "            if activation is not None:\n",
    "                assert isinstance(activation, Module), \\\n",
    "                self.layers.append(activation)\n",
    "            self.layers.append(nn.BatchNorm1d(layer))\n",
    "\n",
    "        self.layers.append(nn.Linear(lin_layers[-1], 2))\n",
    "        \n",
    "        # only one dropout layer after all BN layers inspired by https://arxiv.org/pdf/1801.05134.pdf (also we have a very low amount of training data)\n",
    "        self.layers.append(nn.Dropout(.5))\n",
    "\n",
    "        self.layers.append(out_activation)\n",
    "        \n",
    "        if device == None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, input_data, train=True):\n",
    "        x = test_tensor = torch.cat((input_data[:, 0, :,:], input_data[:,1, :, :]), 2).unsqueeze(1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_training import run_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace82cd9e9804636acb235198527daa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  117 , Accuracy(Training): (70.300%), Accuracy(Test): (50.700%)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rudi/Documents/Uni/Courses-Exchange/DeepLearning/Final_Project/Project_1/Networks.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rudi/Documents/Uni/Courses-Exchange/DeepLearning/Final_Project/Project_1/Networks.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m run_trial(plain_model, \u001b[39m1000\u001b[39;49m, [\u001b[39m200\u001b[39;49m, \u001b[39m200\u001b[39;49m], device)\n",
      "File \u001b[0;32m~/Documents/Uni/Courses-Exchange/DeepLearning/Final_Project/Project_1/run_training.py:67\u001b[0m, in \u001b[0;36mrun_trial\u001b[0;34m(model, epochs, layers, device, loss, optimizer_name, lr)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m     66\u001b[0m     train_accuracy\u001b[39m.\u001b[39mappend(Teacher\u001b[39m.\u001b[39mtrain(NN, epoch))\n\u001b[0;32m---> 67\u001b[0m     test_l, test_acc \u001b[39m=\u001b[39m Teacher\u001b[39m.\u001b[39;49mtest(NN)\n\u001b[1;32m     68\u001b[0m     test_loss\u001b[39m.\u001b[39mappend(test_l)\n\u001b[1;32m     69\u001b[0m     test_accuracy\u001b[39m.\u001b[39mappend(test_acc)\n",
      "File \u001b[0;32m~/Documents/Uni/Courses-Exchange/DeepLearning/Final_Project/Project_1/run_training.py:44\u001b[0m, in \u001b[0;36mteacher.test\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     41\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_data)\n\u001b[1;32m     43\u001b[0m \u001b[39m# compute test loss\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m test_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(output, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_target)\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     45\u001b[0m \u001b[39m# find most likely prediction\u001b[39;00m\n\u001b[1;32m     46\u001b[0m pred \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m1\u001b[39m] \u001b[39m# get the index of the max log-probability\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_trial(plain_model, 1000, [200, 200], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9776258cb6398d27f5227b85aed097ba1d94f1dc0f3390982c9d110f6351e5c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
